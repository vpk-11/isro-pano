{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import statments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "import matplotlib as plt\n",
    "from ultralytics import YOLO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection strings\n",
    "emotion_detection_server_url = f\"http://localhost:5000/process_frame\"\n",
    "hand_gesture_recognition_server_url = f\"http://localhost:5000/process_frame\"\n",
    "finger_counting_server_url = f\"http://localhost:5000/process_frame\"\n",
    "body_posture_recognition_server_url = f\"http://localhost:5000/process_frame\"\n",
    "face_recognition_server_url = f\"http://localhost:5000/process_frame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion that calls a flask server to process frame and return the detected emotion\n",
    "def detectEmotionFlask(frame):\n",
    "    # Encode the frame to JPEG format for transmission\n",
    "    _, encoded_image = cv2.imencode('.jpg', frame)\n",
    "    \n",
    "    # Define the Flask server URL\n",
    "    server_url = emotion_detection_server_url\n",
    "    \n",
    "    try:\n",
    "        # Send the frame to the Flask server\n",
    "        response = requests.post(server_url, files={\"image\": encoded_image.tobytes()})\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json\n",
    "            emotion = data['emotion']\n",
    "            return emotion  # Return the emotion from the server\n",
    "        else:\n",
    "            print(f\"Server error: {response.status_code} - {response.text}\")\n",
    "            return \"Error: Unable to process frame\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with server: {str(e)}\")\n",
    "        return \"Error: Communication failure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand Gesture Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion that calls a flask server to process frame and return the detected hand gesture\n",
    "def detectHandGestureFlask(frame):\n",
    "    # Encode the frame to JPEG format for transmission\n",
    "    _, encoded_image = cv2.imencode('.jpg', frame)\n",
    "    \n",
    "    # Define the Flask server URL\n",
    "    server_url = hand_gesture_recognition_server_url\n",
    "    \n",
    "    try:\n",
    "        # Send the frame to the Flask server\n",
    "        response = requests.post(server_url, files={\"image\": encoded_image.tobytes()})\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json\n",
    "            gesture = data['gesture']\n",
    "            return gesture  # Return the gesture from the server\n",
    "        else:\n",
    "            print(f\"Server error: {response.status_code} - {response.text}\")\n",
    "            return \"Error: Unable to process frame\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with server: {str(e)}\")\n",
    "        return \"Error: Communication failure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finger Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion that calls a flask server to process frame and return the count of fingers\n",
    "def countFingersFlask(frame):\n",
    "    # Encode the frame to JPEG format for transmission\n",
    "    _, encoded_image = cv2.imencode('.jpg', frame)\n",
    "    \n",
    "    # Define the Flask server URL\n",
    "    server_url = finger_counting_server_url\n",
    "    \n",
    "    try:\n",
    "        # Send the frame to the Flask server\n",
    "        response = requests.post(server_url, files={\"image\": encoded_image.tobytes()})\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json\n",
    "            fingersStatus = data['fingersStatus']\n",
    "            count = data['count']\n",
    "            return fingersStatus, count  # Return the count from the server\n",
    "        else:\n",
    "            print(f\"Server error: {response.status_code} - {response.text}\")\n",
    "            return \"Error: Unable to process frame\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with server: {str(e)}\")\n",
    "        return \"Error: Communication failure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Posture Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion that calls a flask server to process frame and return the detected body posture\n",
    "def detectBodyPostureFlask(frame):\n",
    "    # Encode the frame to JPEG format for transmission\n",
    "    _, encoded_image = cv2.imencode('.jpg', frame)\n",
    "    \n",
    "    # Define the Flask server URL\n",
    "    server_url = body_posture_recognition_server_url\n",
    "    \n",
    "    try:\n",
    "        # Send the frame to the Flask server\n",
    "        response = requests.post(server_url, files={\"image\": encoded_image.tobytes()})\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json\n",
    "            posture = data['posture']\n",
    "            return posture  # Return the posture from the server\n",
    "        else:\n",
    "            print(f\"Server error: {response.status_code} - {response.text}\")\n",
    "            return \"Error: Unable to process frame\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with server: {str(e)}\")\n",
    "        return \"Error: Communication failure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Recognition code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion that calls a flask server to process frame and return the detected person name\n",
    "def recognizeFaceFlask(frame):\n",
    "    # Encode the frame to JPEG format for transmission\n",
    "    _, encoded_image = cv2.imencode('.jpg', frame)\n",
    "    \n",
    "    # Define the Flask server URL\n",
    "    server_url = face_recognition_server_url\n",
    "    \n",
    "    try:\n",
    "        # Send the frame to the Flask server\n",
    "        response = requests.post(server_url, files={\"image\": encoded_image.tobytes()})\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json\n",
    "            emotion = data['emotion']\n",
    "            return emotion  # Return the emotion from the server\n",
    "        else:\n",
    "            print(f\"Server error: {response.status_code} - {response.text}\")\n",
    "            return \"Error: Unable to process frame\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with server: {str(e)}\")\n",
    "        return \"Error: Communication failure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect people on frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model (replace with your model path)\n",
    "yoloModel = YOLO(\"YoloV8/yolov8m.pt\")  # Assuming you have a lightweight YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect persons using YOLOv8\n",
    "def detectPersons(yoloModel, image, inputSize=(640, 640)):\n",
    "\t\"\"\"\n",
    "\tDetects persons using a YOLOv8 model.\n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel (YOLO): YOLOv8 model loaded from ultralytics.\n",
    "\t\timage (np.ndarray): Input image as a NumPy array.\n",
    "\t\tinput_size (tuple, optional): Size to resize the input image. Defaults to (640, 640).\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist: List of bounding boxes for detected persons in format [x1, y1, x2, y2].\n",
    "\t\"\"\"\n",
    "\tresults = yoloModel.predict(image, imgsz = 640,conf=0.5)\n",
    "\n",
    "\tboundingBoxes = []  # List to store all bounding boxes (including non-person)\n",
    "\n",
    "\tfor detection in results:\n",
    "\t\tboxes = detection.boxes\n",
    "\t\t# Process each bounding box\n",
    "\t\tfor box in boxes:\n",
    "\t\t\tif detection.names and detection.names[box.cls[0].item()] != 'person':\n",
    "\t\t\t\tcontinue  # Skip if not \"person\" class\n",
    "\n",
    "\t\t\tif isinstance(box, torch.Tensor):\n",
    "\t\t\t\tbox = box.cpu().numpy()\n",
    "\t\t\txmin, ymin, xmax, ymax = map(int, box.xyxy[0])\n",
    "\n",
    "\t\t\tboundingBoxes.append((xmin, ymin, xmax, ymax))  # Add bounding box to general list\n",
    "\n",
    "\t# Now 'all_detections' contains detailed info for all detections\n",
    "\t# 'boundingBoxes' contains all bounding boxes (including non-person)\n",
    "\treturn boundingBoxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract ROI from frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ROI from the image\n",
    "def extractROI(image, bbox):\n",
    "\t\"\"\"\n",
    "\tExtracts a region of interest (ROI) from the image based on the bounding box.\n",
    "\n",
    "\tArgs:\n",
    "\t\timage (np.ndarray): Input image as a NumPy array.\n",
    "\t\tbbox (tuple): Bounding box coordinates. Can be in either format:\n",
    "\t\t\t- (x, y, width, height)\n",
    "\t\t\t- (xmin, ymin, xmax, ymax)\n",
    "\n",
    "\tReturns:\n",
    "\t\tnp.ndarray: Cropped image containing the ROI.\n",
    "\t\"\"\"\n",
    "\n",
    "\tif len(bbox) == 4:  # Check if format is (xmin, ymin, xmax, ymax)\n",
    "\t\tx, y, xmax, ymax = bbox\n",
    "\t\tw = xmax - x\n",
    "\t\th = ymax - y\n",
    "\telse:  # Assume format is (x, y, width, height)\n",
    "\t\tx, y, w, h = bbox\n",
    "\n",
    "\treturn image[y:y + h, x:x + w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import threading module\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Define the number of worker threads (adjust as needed)\n",
    "num_threads = 5\n",
    "\n",
    "# Create a thread pool using ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(max_workers=num_threads)\n",
    "\n",
    "# Main function to process the image\n",
    "def processROI(roi):\n",
    "\n",
    "\t# Submit tasks to the thread pool\n",
    "\tfuture1 = executor.submit(detectEmotionFlask, roi)\n",
    "\tfuture2 = executor.submit(detectHandGestureFlask, roi)\n",
    "\tfuture3 = executor.submit(countFingersFlask, roi)\n",
    "\tfuture4 = executor.submit(detectBodyPostureFlask, roi)\n",
    "\tfuture5 = executor.submit(recognizeFaceFlask, roi)\n",
    "\n",
    "\t# Wait for all tasks to finish and collect outputs\n",
    "\temotionLabel = future1.result()\n",
    "\tgestureLabel = future2.result()\n",
    "\tfingerStatus, fingerCount = future3.result()\n",
    "\tpostureLabel = future4.result()\n",
    "\tfaceLabel = future5.result()\n",
    "\n",
    "\t# emotionLabel = detectEmotion(roi)\n",
    "\t# print(emotionLabel)\n",
    "\n",
    "\t# gestureLabel = recognizeGesture(roi)\n",
    "\t# print(gestureLabel)\n",
    "\n",
    "\t# fingerStatus, fingerCount = countFingers(roi)\n",
    "\t# print(fingerCount)\n",
    "\n",
    "\t# postureLabel = recognizeBodyPosture(roi)\n",
    "\t# print(postureLabel)\n",
    "\n",
    "\treturn emotionLabel, gestureLabel, fingerCount, postureLabel, faceLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funtion gets image as an input\n",
    "uses YoloV8 to get the bboxes for the ROIs\n",
    "Called Process ROI to get the label's from different models\n",
    "returns the image (with the bounding boxes)\n",
    "\"\"\"\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "\n",
    "def processImage(image, c):\n",
    "\t# Detect persons and get bounding boxes\n",
    "\tperson_bboxes = detectPersons(yoloModel, image)\n",
    "\tprint(\"Detected Persons:\", len(person_bboxes))\n",
    "\tif len(person_bboxes) == 0:\n",
    "\t\treturn image, None, None, None, None, None\n",
    "\t\t\n",
    "\t# Draw the bounding box\t\n",
    "\t# Loop through detected persons and extract ROIs\n",
    "\tcounter = 0\n",
    "\tcopy = image.copy()\n",
    "\tannotator = Annotator(image)\n",
    "\tfor bbox in person_bboxes:\n",
    "\t\tcounter+=1\n",
    "\t\troi = extractROI(copy, bbox)\n",
    "\t\t# cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2) # snippet to draw bounding box\n",
    "\t\troi = cv2.resize(roi, dsize=(512, 512))\n",
    "\t\temotionLabel, gestureLabel, fingerCount, postureLabel, faceLabel = processROI(roi)\n",
    "\t\tif faceLabel == \"unknown\":\n",
    "\t\t\tfaceLabel = f\"Person {counter}\"\n",
    "\t\tannotator.box_label(bbox, faceLabel)  # Annotate the image with class names\n",
    "\t\tplt.imshow(roi)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\ttext = f\"Recognized Person: {faceLabel}\\nEmotion: {emotionLabel}\\nGesture: {gestureLabel}\\nFinger Counts: {fingerCount}\\nPosture: {postureLabel}\"\n",
    "\t\tprint(text)\n",
    "\t\n",
    "\treturn annotator.result(), emotionLabel, gestureLabel, fingerCount, postureLabel, faceLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video capture from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "c = 0\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 640)\n",
    "\n",
    "# Font color, format, and position of emotion detail displayed in frame  \n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale              = 0.5\n",
    "fontColor              = (0,255,0)\n",
    "thickness              = 2\n",
    "lineType               = 1\n",
    "\n",
    "while cap.isOpened():\n",
    "\tret, frame = cap.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\n",
    "\tframe, emotionLabel, gestureLabel, fingerCount, postureLabel, faceLabel = processImage(frame, c)\n",
    "\t# text = f\"Recognized Person: {faceLabel} | Emotion: {emotionLabel} | Gesture: {gestureLabel} | Finger Counts: {fingerCount} | Posture: {postureLabel}\"\n",
    "\n",
    "\t# Display the frame with bounding boxes\n",
    "\tcv2.putText(frame,f\"Recognized Person: {faceLabel}\", (50,100), \n",
    "\t\t font, fontScale, fontColor, thickness, lineType)\n",
    "\t# Display the frame with bounding boxes\n",
    "\tcv2.putText(frame,f\"Emotion: {emotionLabel}\", (50,120), \n",
    "\t\t font, fontScale, fontColor, thickness, lineType)\n",
    "\t # Display the frame with bounding boxes\n",
    "\tcv2.putText(frame,f\"Gesture: {gestureLabel}\", (50,140), \n",
    "\t\t font, fontScale, fontColor, thickness, lineType)\n",
    "\t # Display the frame with bounding boxes\n",
    "\tcv2.putText(frame,f\"Finger Counts: {fingerCount}\", (50,160), \n",
    "\t\t font, fontScale, fontColor, thickness, lineType)\n",
    "\t # Display the frame with bounding boxes\n",
    "\tcv2.putText(frame,f\"Posture: {postureLabel}\", (50,180), \n",
    "\t\t font, fontScale, fontColor, thickness, lineType)\n",
    "\t\n",
    "\t# Draw banner on the frame\n",
    "\t# frame_with_banner = draw_banner_on_frame(frame.copy(), [emotionLabel, gestureLabel, fingerCount, postureLabel, faceLabel])\n",
    "\tcv2.imshow(\"YOLOv8 Person Detection\", frame)\n",
    "\t# cv2.imwrite(f'./test/{c}_img.png', frame)\n",
    "\t \n",
    "   \n",
    "\tc+=1\n",
    "\n",
    "\t# Exit on 'q' press\n",
    "\tif cv2.waitKey(1) == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "# Shut down the thread pool (optional)\n",
    "executor.shutdown()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
